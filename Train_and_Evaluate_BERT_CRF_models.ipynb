{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39931,"status":"ok","timestamp":1667552400243,"user":{"displayName":"Guru Aakash G","userId":"07505285016390291133"},"user_tz":-330},"id":"f8CbbYwKhlMY","outputId":"a30c278a-0c36-47e5-a284-438d0227ceda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchcrf\n","  Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from torchcrf) (1.12.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchcrf) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->torchcrf) (4.1.1)\n","Installing collected packages: torchcrf\n","Successfully installed torchcrf-1.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tokenizers\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 16.8 MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.13.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting huggingface_hub\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 12.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.13.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n","Installing collected packages: huggingface-hub\n","Successfully installed huggingface-hub-0.10.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 28.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: transformers\n","Successfully installed transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 25.4 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 36.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=99d445933359b07eb6565a3dae720aad0bb3844487176e2dc3b14da4749cf51b\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sacremoses\n","Installing collected packages: sacremoses\n","Successfully installed sacremoses-0.0.53\n"]}],"source":["!pip install torchcrf\n","!pip install tokenizers\n","!pip install huggingface_hub\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install sacremoses\n","!pip install pytorch-crf\n","!pip install git+https://github.com/kmkurn/pytorch-crf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20885,"status":"ok","timestamp":1667552421122,"user":{"displayName":"Guru Aakash G","userId":"07505285016390291133"},"user_tz":-330},"id":"1YoW2bPHhscP","outputId":"a9be100d-3a9d-48f9-9bc2-8de2db3984e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"3cTQyip2hzJA"},"source":["# Import modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3zZn7gLhxt7"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torchcrf import CRF\n","from tokenizers import pre_tokenizers\n","from tokenizers.pre_tokenizers import Whitespace, Punctuation\n","\n","import torch\n","import pandas as pd\n","import torch.nn as nn\n","import json\n","import math\n","import logging\n","import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKsoB41Cr453"},"outputs":[],"source":["import sys    \n","path_to_module = '/content/gdrive/MyDrive/IT458_project'\n","sys.path.append(path_to_module)\n","\n","from utils import TRANSFORMER_PATH, LABEL_MAPPING, convert_examples_to_features, get_data\n"]},{"cell_type":"markdown","metadata":{"id":"TyxkcRsGh2ao"},"source":["# Dataset preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbbzjMYPjH50"},"outputs":[],"source":["path = \"/content/gdrive/MyDrive/IT458_project/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3IhUrp0h7XX"},"outputs":[],"source":["class GunViolenceDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, index):\n","        return self.texts[index], self.labels[index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJFc0p4wjbWK"},"outputs":[],"source":["def _gen_label(words, target):\n","    target_len = len(target)\n","    tags = ['O'] * len(words)\n","    for i in range(0, len(words)):\n","        try:\n","            if ' '.join(words[i:i+target_len]) == ' '.join(target):\n","                tags[i] = 'B'\n","                for j in range(i+1, i+target_len):\n","                    tags[j] = 'I'\n","        except IndexError as e:\n","            print(e)\n","            exit()\n","\n","    return ' '.join(tags)\n","\n","def preprocess(input_file, output_file, target_type):\n","    df = pd.read_csv(input_file, sep='\\t')\n","    texts = df['Full text'].tolist()\n","    jsons = df['Json'].tolist()\n","\n","    new_texts = []\n","    labels = []\n","\n","    for text, data in zip(texts, jsons):\n","        try:\n","            # use BERT tokenizer to process whitespace and punctuaction\n","            data = json.loads(data)\n","            text = text.replace('\\u200b', '')\n","            pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation()])\n","            tokenized_text = [t[0] for t in pre_tokenizer.pre_tokenize_str(text)]\n","            target = data[target_type + '-section'][0]['name']['value']\n","            tokenized_target = [t[0] for t in pre_tokenizer.pre_tokenize_str(target)]\n","\n","            # if no target or empty in array, mark every token as O\n","            if not target:\n","                raise IndexError\n","\n","            # generate labels for each tokenized token\n","            label = _gen_label(tokenized_text, tokenized_target)\n","\n","            # keep tokenized text that has less than 512 text-length\n","            if label and len(tokenized_text) < 512:\n","                new_texts.append(' '.join(tokenized_text))\n","                labels.append(label)\n","\n","        except IndexError:\n","            # mark every token as O\n","            new_texts.append(' '.join(tokenized_text))\n","            label = ' '.join(['O'] * len(tokenized_text))\n","            labels.append(label)\n","\n","    data = list(zip(new_texts, labels))\n","    df = pd.DataFrame(data)\n","    df.columns = ['texts', 'labels']\n","    df.to_csv(output_file, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmMzPjyTjobg"},"outputs":[],"source":["target = \"victim\"\n","\n","preprocess(path + 'dataset/train.tsv', path + target + '/train.csv', target)\n","preprocess(path + 'dataset/dev.tsv', path + target + '/dev.csv', target)\n","preprocess(path + 'dataset/test.tsv', path + target + '/test.csv', target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xc6bo7gjoe8"},"outputs":[],"source":["target = \"shooter\"\n","\n","preprocess(path + 'dataset/train.tsv', path + target + '/train.csv', target)\n","preprocess(path + 'dataset/dev.tsv', path + target + '/dev.csv', target)\n","preprocess(path + 'dataset/test.tsv', path + target + '/test.csv', target)"]},{"cell_type":"markdown","metadata":{"id":"1lkAzkaIiG5X"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"bjAKJbhGiJVN"},"source":["### BERT_CRF_Linear"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHjqOTpJiM72"},"outputs":[],"source":["class BERT_CRF_Linear(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BERT_CRF_Linear, self).__init__()\n","        config = torch.hub.load(TRANSFORMER_PATH, 'config', 'bert-base-cased')\n","        config.max_position_embeddings = 1024\n","        self.bert = torch.hub.load(TRANSFORMER_PATH, 'model', 'bert-base-cased')\n","        self.classifier = nn.Linear(768, num_labels)\n","        self.CRF_model = CRF(num_labels, batch_first=True)\n","\n","    def forward(self, tokens_tensor, segments_tensors, labels=None):\n","        bert_output = self.bert(tokens_tensor, token_type_ids=segments_tensors)\n","        last_hidden_state = bert_output.last_hidden_state\n","        pooler_output = bert_output.pooler_output\n","\n","        logits = self.classifier(last_hidden_state)\n","\n","        # the CRF layer of NER labels\n","        crf_loss_list = self.CRF_model(logits, labels)\n","        crf_loss = torch.mean(-crf_loss_list)\n","        crf_predict = self.CRF_model.decode(logits)\n","\n","\t\t# the classifier of category & polarity\n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits.permute(0, 2, 1), labels)\n","        return torch.tensor(crf_predict).to('cuda'), logits, loss"]},{"cell_type":"markdown","metadata":{"id":"oBo1-w7OiRWy"},"source":["### BERT_CRF_LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKyV38Z2iRhC"},"outputs":[],"source":["class BERT_CRF_LSTM(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BERT_CRF_LSTM, self).__init__()\n","        config = torch.hub.load(TRANSFORMER_PATH, 'config', 'bert-base-cased')\n","        config.max_position_embeddings = 1024\n","        self.bert = torch.hub.load(TRANSFORMER_PATH, 'model', 'bert-base-cased')\n","        self.lstm = nn.LSTM(768, 768)\n","        self.classifier = nn.Linear(768, num_labels)\n","        self.CRF_model = CRF(num_labels, batch_first=True)\n","\n","    def forward(self, tokens_tensor, segments_tensors, labels=None):\n","        bert_output = self.bert(tokens_tensor, token_type_ids=segments_tensors)\n","        last_hidden_state = bert_output.last_hidden_state\n","        pooler_output = bert_output.pooler_output\n","\n","        lstm_out, _ = self.lstm(last_hidden_state)\n","        logits = self.classifier(lstm_out)\n","\n","        # the CRF layer of NER labels\n","        crf_loss_list = self.CRF_model(logits, labels)\n","        crf_loss = torch.mean(-crf_loss_list)\n","        crf_predict = self.CRF_model.decode(logits)\n","\n","\t\t# the classifier of category & polarity\n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits.permute(0, 2, 1), labels)\n","        return torch.tensor(crf_predict).to('cuda'), logits, loss"]},{"cell_type":"markdown","metadata":{"id":"Hoh7m_xviRrR"},"source":["### BERT_CRF_BiLSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EdWtnD5iR1T"},"outputs":[],"source":["class BERT_CRF_BiLSTM(nn.Module):\n","    def __init__(self, num_labels):\n","        super(BERT_CRF_BiLSTM, self).__init__()\n","        config = torch.hub.load(TRANSFORMER_PATH, 'config', 'bert-base-cased')\n","        config.max_position_embeddings = 1024\n","        self.bert = torch.hub.load(TRANSFORMER_PATH, 'model', 'bert-base-cased')\n","        self.lstm = nn.LSTM(768, 768, bidirectional=True)\n","        self.classifier = nn.Linear(768, num_labels)\n","        # self.classifier = nn.Linear(768 * 2, num_labels)\n","        self.CRF_model = CRF(num_labels, batch_first=True)\n","\n","    def forward(self, tokens_tensor, segments_tensors, labels=None):\n","        bert_output = self.bert(tokens_tensor, token_type_ids=segments_tensors)\n","        last_hidden_state = bert_output.last_hidden_state\n","        pooler_output = bert_output.pooler_output\n","\n","        lstm_out, _ = self.lstm(last_hidden_state)\n","        lstm_out = lstm_out[:, :, :768] + lstm_out[:, :, 768:]\n","\n","        logits = self.classifier(lstm_out)\n","\n","        # the CRF layer of NER labels\n","        crf_loss_list = self.CRF_model(logits, labels)\n","        crf_loss = torch.mean(-crf_loss_list)\n","        crf_predict = self.CRF_model.decode(logits)\n","\n","\t\t# the classifier of category & polarity\n","        loss_fct = nn.CrossEntropyLoss()\n","        loss = loss_fct(logits.permute(0, 2, 1), labels)\n","        return torch.tensor(crf_predict).to('cuda'), logits, loss"]},{"cell_type":"markdown","metadata":{"id":"_5ufWuE1ohGd"},"source":["# Train and Evaluate model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMlY0yyK-fPd"},"outputs":[],"source":["def evaluate(model, evaluate_X, evaluate_Y, tokenizer, cuda_available, batch_size, max_seq_length, model_type, lr, epochs, path, get_accuracy):\n","\n","    def _get_prediction(normalized_probs):\n","        # classify B, I, O based on probabilities\n","        labels = []\n","        for sample_prob in normalized_probs:\n","            max_prob = -math.inf\n","            label = None\n","            for i, prob in enumerate(sample_prob):\n","                if max_prob < prob:\n","                    max_prob = prob\n","                    label = i\n","            labels.append(label)\n","        return labels\n","\n","    model.eval()\n","    num_samples = len(evaluate_X)\n","    evaluate_set = GunViolenceDataset(evaluate_X, evaluate_Y)\n","    evaluate_generator = DataLoader(\n","        evaluate_set,\n","        batch_size=1,\n","        shuffle=True,\n","    )\n","    num_of_tp = num_of_fn = num_of_fp = num_of_tn = 0\n","\n","    for i, (evaluate_x, evaluate_y) in enumerate(evaluate_generator):\n","        tokens, labels = convert_examples_to_features(evaluate_x, evaluate_y, tokenizer, max_seq_length)\n","\n","        indexed_tokens = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n","        segments_ids = [[0] * len(indexed_token) for indexed_token in indexed_tokens]\n","\n","        if cuda_available:\n","            segments_tensors = torch.tensor(segments_ids).to('cuda')\n","            tokens_tensor = torch.tensor(indexed_tokens).to('cuda')\n","            labels = torch.tensor(labels).to('cuda')\n","        else:\n","            segments_tensors = torch.tensor(segments_ids)\n","            tokens_tensor = torch.tensor(indexed_tokens)\n","            labels = torch.tensor(labels)\n","\n","        with torch.no_grad():\n","            y_pred, logits, loss = model(tokens_tensor, segments_tensors, labels)\n","            normalized_probs = nn.functional.softmax(logits, dim=1)[0]\n","            results = y_pred[0]\n","\n","            # get the real target\n","            original = ''\n","            for i, (x, y) in enumerate(zip(evaluate_x[0].split(), evaluate_y[0].split())):\n","                if y[0] == 'B':\n","                    original = x + ' '\n","                    index = i\n","                    while index + 1 < len(evaluate_y[0].split()) and evaluate_y[0].split()[index + 1][0] == 'I':\n","                        original += '{} '.format(evaluate_x[0].split()[index + 1])\n","                        index += 1\n","                    break\n","            original = original.strip()\n","\n","            probabilities = []\n","            predictions = []\n","            prediction = []\n","\n","            for token, tag, prob in zip(tokens[0], results, normalized_probs):\n","                if tag == 0:\n","                    # tag == 'B'\n","                    probabilities.append(prob)\n","\n","                    if len(prediction) != 0:\n","                        predictions.append(prediction)\n","                        prediction = []\n","                    prediction.append(token)\n","                elif tag == 1:\n","                    # tag == 'I'\n","                    prediction.append(token)\n","            if len(prediction) != 0:\n","                predictions.append(prediction)\n","\n","            # one sentence might generate multiple targets, eg. shooters or victims\n","            # we need to pick the most possible one, which is the one has the highest probability in 'B' tag\n","            max_prob = -math.inf\n","            max_prob_ind = 0\n","            for i, prob in enumerate(probabilities):\n","                if max_prob < prob[0]:\n","                    max_prob_ind = i\n","                    max_prob = prob[0]\n","\n","            # calculate true positive, false positive, true negative, false negative\n","            result = ''\n","            if len(predictions) != 0:\n","                result = tokenizer.convert_tokens_to_string(predictions[max_prob_ind])\n","                if result == original:\n","                    num_of_tp += 1\n","                else:\n","                    num_of_fp += 1\n","            else:\n","                if original.strip() != '':\n","                    num_of_fn += 1\n","                else:\n","                    num_of_tn += 1\n","\n","    accuracy = num_of_tp/num_samples if num_samples != 0 else 0\n","    precision = num_of_tp/(num_of_tp + num_of_fp) if num_of_tp + num_of_fp != 0 else 0\n","    recall = num_of_tp/(num_of_tp + num_of_fn) if num_of_tp + num_of_fn != 0 else 0\n","\n","    if(get_accuracy):\n","      print('Accuracy : {}'.format(accuracy))\n","    else:\n","      print(\"\\nEvaluation :\\n\")\n","      print('True positive : {}'.format(num_of_tp))\n","      print('False positive : {}'.format(num_of_tp))\n","      print('True negative : {}'.format(num_of_tp))\n","      print('False negative : {}'.format(num_of_tp))\n","      print('Accuracy : {}'.format(accuracy))\n","      print('Precision : {}'.format(precision))\n","      print('Recall : {}'.format(recall))\n","      print('F1_score : {}'.format(2 * precision * recall / (precision + recall) if precision + recall != 0 else 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AoSgjsEaudnz"},"outputs":[],"source":["def train(train_X, train_Y, learning_rate, cuda_available, epochs, model_type, is_balance, batch_size, max_seq_length, patience, min_delta, baseline, path, evaluate_X, evaluate_Y, lr):\n","\n","    training_set = GunViolenceDataset(train_X, train_Y)\n","    training_generator = DataLoader(\n","        training_set,\n","        batch_size=batch_size,\n","        shuffle=True,\n","    )\n","    iter_in_one_epoch = len(train_X) // batch_size\n","\n","    tokenizer = torch.hub.load(TRANSFORMER_PATH, 'tokenizer', 'bert-base-cased') # cased!\n","    model = None\n","    if model_type == 'LSTM':\n","        model = BERT_CRF_LSTM(3)\n","    elif model_type == 'BiLSTM':\n","        model = BERT_CRF_BiLSTM(3)\n","    else:\n","        model = BERT_CRF_Linear(3)  # 3 different labels: B, I, O\n","\n","    if cuda_available:\n","        model.to('cuda')  # move data onto GPU\n","\n","    model.train()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    losses = []\n","    num_no_improve = 0\n","    best_loss = None\n","    stopping_epoch = 0\n","\n","    for epoch in range(1, epochs + 1):\n","        loss = 0\n","        with tqdm.tqdm(training_generator, unit=\"batch\") as tepoch:\n","            for i, (train_x, train_y) in enumerate(tepoch):\n","                tepoch.set_description(f\"Epoch {epoch}\")\n","\n","                # prepare model input\n","                tokens, labels = convert_examples_to_features(train_x, train_y, tokenizer, max_seq_length)\n","                indexed_tokens = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n","                segments_ids = [[0] * len(indexed_token) for indexed_token in indexed_tokens]\n","\n","                if cuda_available:\n","                    segments_tensors = torch.tensor(segments_ids).to('cuda')\n","                    tokens_tensor = torch.tensor(indexed_tokens).to('cuda')\n","                    labels = torch.tensor(labels).to('cuda')\n","                else:\n","                    segments_tensors = torch.tensor(segments_ids)\n","                    tokens_tensor = torch.tensor(indexed_tokens)\n","                    labels = torch.tensor(labels)\n","\n","                # forward pass\n","                y_pred, logits, loss = model(tokens_tensor, segments_tensors, labels)\n","                losses.append((epoch + i / iter_in_one_epoch, loss.item()))\n","\n","                # display loss\n","                tepoch.set_postfix(loss=\"{:.4f}\".format(loss.item()))\n","\n","                # zero out gradients\n","                optimizer.zero_grad()\n","\n","                # backward pass\n","                loss.backward()\n","\n","                # update parameters\n","                optimizer.step()\n","\n","            if not best_loss:\n","                best_loss = loss\n","            elif loss <= best_loss + min_delta:\n","                best_loss = loss\n","                num_no_improve += 1\n","            elif loss < baseline:\n","                num_no_improve += 1\n","            if num_no_improve > patience:\n","                stopping_epoch = epoch\n","                logging.info('Early Stop on epoch {} with the best loss {}'.format(stopping_epoch, best_loss))\n","                break\n","\n","        torch.save(model, path + model_type + \"_model\")\n","        #evaluate(model, evaluate_X, evaluate_Y, tokenizer, cuda_available, batch_size, max_seq_length, model_type, lr, epochs, path, True)\n","\n","    return model, tokenizer, stopping_epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpBx7-XH9uwD"},"outputs":[],"source":["lr = 1e-4\n","epochs = 7\n","batch_size = 40\n","max_seq_length = 256\n","is_balance = True\n","patience = 10\n","min_delta = 0\n","baseline = 0.0001\n","cuda_available = torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"_cwc_d9F_XFh"},"source":["### For victim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjXu9C22--dc"},"outputs":[],"source":["input_dir = path + \"victim\"\n","output_dir = path + \"victim/\"\n","train_X, train_Y = get_data(input_dir + '/train.csv', is_balance)\n","dev_X, dev_Y = get_data(input_dir + '/dev.csv', is_balance)\n","test_X, test_Y = get_data(input_dir + '/test.csv')\n","train_X += dev_X\n","train_Y += dev_Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8JC7Zal9rdn"},"outputs":[],"source":["model, tokenizer, stopping_epoch = train(train_X, train_Y, lr, cuda_available, epochs, \"Linear\", is_balance, batch_size, max_seq_length, patience, min_delta, baseline, output_dir, test_X, test_Y, lr )\n","evaluate(model, test_X, test_Y, tokenizer, cuda_available, batch_size, max_seq_length, \"Linear\", lr, stopping_epoch, output_dir, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4Y0O6Wq9AmQ"},"outputs":[],"source":["model, tokenizer, stopping_epoch = train(train_X, train_Y, lr, cuda_available, epochs, \"LSTM\", is_balance, batch_size, max_seq_length, patience, min_delta, baseline, output_dir, test_X, test_Y, lr )\n","evaluate(model, test_X, test_Y, tokenizer, cuda_available, batch_size, max_seq_length, \"LSTM\", lr, stopping_epoch, output_dir, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t46XdrBV-sCo"},"outputs":[],"source":["model, tokenizer, stopping_epoch = train(train_X, train_Y, lr, cuda_available, epochs, \"BiLSTM\", is_balance, batch_size, max_seq_length, patience, min_delta, baseline, output_dir, test_X, test_Y, lr )\n","evaluate(model, test_X, test_Y, tokenizer, cuda_available, batch_size, max_seq_length, \"BiLSTM\", lr, stopping_epoch, output_dir, False)"]},{"cell_type":"markdown","metadata":{"id":"lNRM9zpn_bK8"},"source":["### For shooter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clhKzE_j_EAn"},"outputs":[],"source":["input_dir = path + \"shooter\"\n","output_dir = path + \"shooter/\"\n","train_X, train_Y = get_data(input_dir + '/train.csv', is_balance)\n","dev_X, dev_Y = get_data(input_dir + '/dev.csv', is_balance)\n","test_X, test_Y = get_data(input_dir + '/test.csv')\n","train_X += dev_X\n","train_Y += dev_Y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0g4G_mW6_IJV"},"outputs":[],"source":["model, tokenizer, stopping_epoch = train(train_X, train_Y, lr, cuda_available, epochs, \"Linear\", is_balance, batch_size, max_seq_length, patience, min_delta, baseline, output_dir, test_X, test_Y, lr )\n","evaluate(model, test_X, test_Y, tokenizer, cuda_available, batch_size, max_seq_length, \"Linear\", lr, stopping_epoch, output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asB0TSi9_ItV"},"outputs":[],"source":["model, tokenizer, stopping_epoch = train(train_X, train_Y, lr, cuda_available, epochs, \"LSTM\", is_balance, batch_size, max_seq_length, patience, min_delta, baseline, output_dir, test_X, test_Y, lr )\n","evaluate(model, test_X, test_Y, tokenizer, cuda_available, batch_size, max_seq_length, \"LSTM\", lr, stopping_epoch, output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9YcI9vV_J_y"},"outputs":[],"source":["model, tokenizer, stopping_epoch = train(train_X, train_Y, lr, cuda_available, epochs, \"BiLSTM\", is_balance, batch_size, max_seq_length, patience, min_delta, baseline, output_dir, test_X, test_Y, lr )\n","evaluate(model, test_X, test_Y, tokenizer, cuda_available, batch_size, max_seq_length, \"BiLSTM\", lr, stopping_epoch, output_dir)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM2iCAD8SzFypIpELk2OvQj","collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10"},"vscode":{"interpreter":{"hash":"f61cae94f334f6933d5b3874029c36b92a0988c5abaf22d4f8c9ffa35b94cf1d"}}},"nbformat":4,"nbformat_minor":0}
